# ============================================================================
# SkyRL Development Configuration - FASRC Test Partitions
# ============================================================================
# Optimized for Harvard FASRC cluster test partitions:
# - GPU: gpu_test partition with MIG A100 GPUs (8x 20GB slices per node)
# - CPU: test partition for runtime server
# - Time Limit: 12 hours maximum
#
# MIG GPU Notes:
# - Each MIG slice is ~20GB (A100 40GB split)
# - Qwen3-1.7B (~3.4GB) fits comfortably in one MIG slice
# - SLURM handles MIG assignment via --gres=gpu:N
#
# Podman Notes (FASRC uses Podman with Docker alias):
# - Before submitting jobs, start Podman socket:
#     podman system service unix:///tmp/podman.sock --time=0 &
#     export DOCKER_HOST=unix:///tmp/podman.sock
# - Verify with: docker run --rm hello-world
# ============================================================================

# ----------------------------------------------------------------------------
# SLURM Partition Configuration
# ----------------------------------------------------------------------------
GPU_PARTITION="gpu_test"
CPU_PARTITION="test"

# ----------------------------------------------------------------------------
# Resource Allocation (scaled for MIG GPUs and test partitions)
# ----------------------------------------------------------------------------

# GPU Training Job
# - Using 1 MIG slice (20GB) due to Ray MIG detection issue
# - Ray's available_resources_per_node() only detects 1 GPU with MIG
# - TODO: Investigate Ray MIG compatibility for multi-GPU support
GPU_NODES=1
GPUS_PER_NODE=1
TRAINING_CPUS=8
TRAINING_MEMORY="32G"

# CPU Runtime Job
# - Reduced from production values for test partition limits
CPUS_FOR_RUNTIME=32
RUNTIME_MEMORY="64G"

# Per-worker allocation (runtime containers)
CPUS_PER_WORKER=8
MEMORY_PER_WORKER="16G"

# Maximum parallel agents (reduced for smaller resource allocation)
MAX_PARALLEL_AGENTS=4

# ----------------------------------------------------------------------------
# Model Configuration
# ----------------------------------------------------------------------------
# Qwen3-1.7B fits well in 20GB MIG slice with room for KV cache
MODEL="Qwen/Qwen3-1.7B"
TP_SIZE=1    # Single MIG slice per model instance
SP_SIZE=1    # No sequence parallelism needed for small model

# ----------------------------------------------------------------------------
# Training Configuration (scaled down for faster test iteration)
# ----------------------------------------------------------------------------
TRAIN_BATCH_SIZE=4
ROLLOUT_N=8   # Must match generator.num_trajectories in verl_oh.yaml
MAX_PROMPT_LENGTH=4096
MAX_RESPONSE_LENGTH=8192
TOTAL_EPOCHS=1

# ----------------------------------------------------------------------------
# Time Limits
# ----------------------------------------------------------------------------
# Test partitions allow up to 12 hours
TIME_LIMIT="12:00:00"

# Timeout for runtime ready signal (seconds)
RUNTIME_READY_TIMEOUT=600

# ----------------------------------------------------------------------------
# Data Paths
# ----------------------------------------------------------------------------
# Using FASRC netscratch location
DATA_DIR="/n/netscratch/janapa_reddi_lab/Lab/jjma/data/r2e-all"
TRAIN_DATA="${DATA_DIR}/train.parquet"
VAL_DATA="${DATA_DIR}/validation.parquet"

# ----------------------------------------------------------------------------
# Output Directories
# ----------------------------------------------------------------------------
CHECKPOINT_DIR="/n/netscratch/janapa_reddi_lab/Lab/jjma/ckpts/skyrl-dev"
ROLLOUT_DIR="/n/netscratch/janapa_reddi_lab/Lab/jjma/rollouts/skyrl-dev"

# ----------------------------------------------------------------------------
# Logging / Monitoring
# ----------------------------------------------------------------------------
WANDB_PROJECT="skyrl-dev"
WANDB_EXPERIMENT="skyrl-dev-test"

# Enable debug logging
SKYRL_DEBUG=1

# ----------------------------------------------------------------------------
# Container Configuration
# ----------------------------------------------------------------------------
CONTAINER_IMAGE="ghcr.io/openhands/agent-server"
RUNTIME_PORT=8000

# ----------------------------------------------------------------------------
# MIG GPU / vLLM Configuration
# ----------------------------------------------------------------------------
# GPU memory utilization - leave headroom for MIG overhead
GPU_MEMORY_UTILIZATION=0.85

# Enable vLLM V1 for better performance
VLLM_USE_V1=1

# ----------------------------------------------------------------------------
# Network Configuration (FASRC test partitions)
# ----------------------------------------------------------------------------
# Test partitions don't have EFA - use TCP instead
FI_PROVIDER="tcp"

# Disable LD_LIBRARY_PATH export (not needed without EFA)
SKYRL_LD_LIBRARY_PATH_EXPORT=0
