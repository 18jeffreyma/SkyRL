# ============================================================================
# SkyRL Development Configuration - FASRC Test Partitions
# ============================================================================
# Optimized for Harvard FASRC cluster test partitions:
# - GPU: gpu_test partition with MIG A100 GPUs (8x 20GB slices per node)
# - CPU: test partition for runtime server
# - Time Limit: 12 hours maximum
#
# MIG GPU Notes:
# - Each MIG slice is ~20GB (A100 40GB split)
# - Qwen3-1.7B (~3.4GB) fits comfortably in one MIG slice
# - SLURM handles MIG assignment via --gres=gpu:N
#
# Podman Notes (FASRC uses Podman with Docker alias):
# - Before submitting jobs, start Podman socket:
#     podman system service unix:///tmp/podman.sock --time=0 &
#     export DOCKER_HOST=unix:///tmp/podman.sock
# - Verify with: docker run --rm hello-world
# ============================================================================

# ----------------------------------------------------------------------------
# SLURM Partition Configuration
# ----------------------------------------------------------------------------
GPU_PARTITION="gpu_test"
CPU_PARTITION="test"

# ----------------------------------------------------------------------------
# Resource Allocation (scaled for MIG GPUs and test partitions)
# ----------------------------------------------------------------------------

# GPU Training Job
# - Using 1 MIG slice (20GB) due to Ray MIG detection issue
# - Ray's available_resources_per_node() only detects 1 GPU with MIG
# - TODO: Investigate Ray MIG compatibility for multi-GPU support
GPU_NODES=1
GPUS_PER_NODE=1
TRAINING_CPUS=8
TRAINING_MEMORY="32G"

# CPU Runtime Job
# - Reduced from production values for test partition limits
CPUS_FOR_RUNTIME=32
RUNTIME_MEMORY="64G"

# Per-worker allocation (runtime containers)
# Reduced to 4 CPUs to allow more parallel workers within 32-CPU allocation
CPUS_PER_WORKER=4
MEMORY_PER_WORKER="8G"

# Maximum parallel agents
# With 32 CPUs and 4 CPUs per worker (4 reserved for builds), we get (32-4)/4 = 7 slots
MAX_PARALLEL_AGENTS=7

# ----------------------------------------------------------------------------
# Model Configuration
# ----------------------------------------------------------------------------
# Qwen2.5-Coder-0.5B-Instruct: 32K context window, ~1GB VRAM base
# Smaller model to avoid OOM on 20GB MIG slice during training
# (Qwen2.5-1.5B was OOM, Qwen2.5-3B was OOM, Qwen3-1.7B had only 12K context)
MODEL="Qwen/Qwen2.5-Coder-0.5B-Instruct"
TP_SIZE=1    # Single MIG slice per model instance
SP_SIZE=1    # No sequence parallelism needed for small model

# ----------------------------------------------------------------------------
# Training Configuration (scaled down for faster test iteration)
# ----------------------------------------------------------------------------
# With 7 worker slots available, we can run batch_size * rollout_n = 1 * 2 = 2 parallel agents
# Keep small for testing, increase batch_size on production clusters with more resources
TRAIN_BATCH_SIZE=1
ROLLOUT_N=2   # Must match generator.num_trajectories in verl_oh.yaml
MAX_PROMPT_LENGTH=2048   # Reduced aggressively for 20GB MIG GPU backward pass
MAX_RESPONSE_LENGTH=1024 # Reduced aggressively to avoid OOM during backward pass
TOTAL_EPOCHS=1

# vLLM max model length - reduced to minimize KV cache memory usage
# Qwen2.5-Coder-0.5B supports 32K but we use 4K to leave more room for training gradients
MAX_MODEL_LEN=4096

# ----------------------------------------------------------------------------
# Time Limits
# ----------------------------------------------------------------------------
# Reduced to 6 hours to improve queue scheduling priority
# (shorter jobs get scheduled faster on gpu_test)
TIME_LIMIT="06:00:00"

# Timeout for runtime ready signal (seconds)
RUNTIME_READY_TIMEOUT=600

# ----------------------------------------------------------------------------
# Data Paths
# ----------------------------------------------------------------------------
# Using FASRC netscratch location
DATA_DIR="/n/netscratch/janapa_reddi_lab/Lab/jjma/data/r2e-all"
TRAIN_DATA="${DATA_DIR}/train.parquet"
VAL_DATA="${DATA_DIR}/validation.parquet"

# ----------------------------------------------------------------------------
# Output Directories
# ----------------------------------------------------------------------------
CHECKPOINT_DIR="/n/netscratch/janapa_reddi_lab/Lab/jjma/ckpts/skyrl-dev"
ROLLOUT_DIR="/n/netscratch/janapa_reddi_lab/Lab/jjma/rollouts/skyrl-dev"

# ----------------------------------------------------------------------------
# Logging / Monitoring
# ----------------------------------------------------------------------------
WANDB_PROJECT="skyrl-dev"
WANDB_EXPERIMENT="skyrl-dev-test"

# Enable debug logging
SKYRL_DEBUG=1

# ----------------------------------------------------------------------------
# Container Configuration
# ----------------------------------------------------------------------------
CONTAINER_IMAGE="ghcr.io/openhands/agent-server"
RUNTIME_PORT=8000

# ----------------------------------------------------------------------------
# MIG GPU / vLLM Configuration
# ----------------------------------------------------------------------------
# GPU memory utilization - reduced from 0.85 to leave room for KV cache
GPU_MEMORY_UTILIZATION=0.5  # Reduced to leave more memory for training gradients

# Enable vLLM V1 for better performance
VLLM_USE_V1=1

# ----------------------------------------------------------------------------
# Network Configuration (FASRC test partitions)
# ----------------------------------------------------------------------------
# Test partitions don't have EFA - use TCP instead
FI_PROVIDER="tcp"

# Disable LD_LIBRARY_PATH export (not needed without EFA)
SKYRL_LD_LIBRARY_PATH_EXPORT=0

