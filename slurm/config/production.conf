# ============================================================================
# SkyRL Production Configuration
# ============================================================================
# Large-scale configuration for production training runs.
# - 4 GPU nodes with 8 GPUs each (32 total GPUs)
# - 96+ parallel agents
# - Longer time limits for full training
# ============================================================================

# ----------------------------------------------------------------------------
# SLURM Partition Configuration
# ----------------------------------------------------------------------------
# Adjust these based on your cluster's partition names
GPU_PARTITION="gpu_quad"
CPU_PARTITION="cpu_population"

# ----------------------------------------------------------------------------
# Resource Allocation
# ----------------------------------------------------------------------------

# GPU Training Job
GPU_NODES=4
GPUS_PER_NODE=8
TRAINING_CPUS=96
TRAINING_MEMORY="1000G"

# CPU Runtime Job
CPUS_FOR_RUNTIME=128
RUNTIME_MEMORY="512G"

# Per-worker allocation (runtime containers)
CPUS_PER_WORKER=8
MEMORY_PER_WORKER="16G"

# Maximum parallel agents (128 CPUs / 8 CPUs per worker = 16 workers max)
# But with TRAIN_BATCH_SIZE=64 and ROLLOUT_N=8, we need batch_size * rollout_n = 512 parallel
# So we need multiple runtime nodes or careful scheduling
MAX_PARALLEL_AGENTS=96

# ----------------------------------------------------------------------------
# Model Configuration
# ----------------------------------------------------------------------------
MODEL="Qwen/Qwen3-32B"
TP_SIZE=4
SP_SIZE=4

# ----------------------------------------------------------------------------
# Training Configuration
# ----------------------------------------------------------------------------
TRAIN_BATCH_SIZE=64
ROLLOUT_N=8
MAX_PROMPT_LENGTH=8000
MAX_RESPONSE_LENGTH=32768
TOTAL_EPOCHS=15

# ----------------------------------------------------------------------------
# Time Limits
# ----------------------------------------------------------------------------
TIME_LIMIT="12:00:00"

# Timeout for runtime ready signal (seconds)
RUNTIME_READY_TIMEOUT=900

# ----------------------------------------------------------------------------
# Data Paths
# ----------------------------------------------------------------------------
DATA_DIR="/mnt/shared_storage/datasets/r2e-all"
TRAIN_DATA="${DATA_DIR}/train.parquet"
VAL_DATA="${DATA_DIR}/validation.parquet"

# ----------------------------------------------------------------------------
# Output Directories
# ----------------------------------------------------------------------------
CHECKPOINT_DIR="/mnt/local_storage/ckpts/skyrl-production"
ROLLOUT_DIR="/mnt/local_storage/rollouts/skyrl-production"

# ----------------------------------------------------------------------------
# Logging / Monitoring
# ----------------------------------------------------------------------------
WANDB_PROJECT="skyrl-production"
# Experiment name will be auto-generated with timestamp

# Disable debug logging for performance
SKYRL_DEBUG=0

# ----------------------------------------------------------------------------
# Container Configuration
# ----------------------------------------------------------------------------
CONTAINER_IMAGE="ghcr.io/openhands/agent-server"
RUNTIME_PORT=8000

# ----------------------------------------------------------------------------
# Advanced Training Options
# ----------------------------------------------------------------------------
# Gradient checkpointing (helps with memory for large models)
ENABLE_GRADIENT_CHECKPOINTING=true

# FSDP offloading
FSDP_PARAM_OFFLOAD=true
FSDP_OPTIMIZER_OFFLOAD=true

# vLLM configuration
VLLM_USE_V1=1
GPU_MEMORY_UTILIZATION=0.8

# EFA networking for multi-node
SKYRL_LD_LIBRARY_PATH_EXPORT=1
FI_PROVIDER=efa
